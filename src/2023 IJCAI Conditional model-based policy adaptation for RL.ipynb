{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a617f86",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e120aebf",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import notebook_setup\n",
    "from copy import deepcopy\n",
    "import warnings, shutil, os, pickle\n",
    "from types import SimpleNamespace\n",
    "from tqdm.auto import tqdm, trange\n",
    "import control\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.functional import jacobian, hessian\n",
    "from commonml.helpers.logs import get_tensorboard_scalar_frame\n",
    "from commonml.rl.ppo import returns\n",
    "from commonml.stats.agg import mean_std\n",
    "\n",
    "from systems.base import SystemEnv\n",
    "from systems.plotting import (\n",
    "    plot_env_response,\n",
    "    multiple_response_plots\n",
    ")\n",
    "from systems.springmass import create_springmass, SpringMassEnv\n",
    "from systems.lunarlander import LanderEnv\n",
    "\n",
    "from rl import learn_rl, transform_rl_policy, evaluate_rl\n",
    "from xform import (\n",
    "    policy_transform, action_transform, get_transforms,\n",
    "    pseudo_matrix, pseudo_matrix_from_data, ab_xform_from_pseudo_matrix,\n",
    "    get_env_samples,\n",
    "    dpolicy_dfa, dpolicy_dfb, err_inv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af0581",
   "metadata": {},
   "source": [
    "## System specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3095b6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SpringMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529371b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NonLinearMatrix:\n",
    "    def __init__(self, orig, func):\n",
    "        self.orig = orig\n",
    "        self.func = func\n",
    "    def __matmul__(self, *args):\n",
    "        x=args[0]\n",
    "        new_matrix = self.func(self.orig.copy(), x)\n",
    "        return new_matrix @ x\n",
    "def make_xform(fraction, position):\n",
    "    def xform(A, x):\n",
    "        if abs(x[0]) <= position:\n",
    "            A[1,0] *= fraction**(1-abs(x[0]-position)/position)\n",
    "        return A\n",
    "    return xform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a4fb1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys_kwargs = dict(k=4, m=0.2, df=0.01)\n",
    "learn_kwargs = dict(steps=50_000, seed=0, learning_rate=2e-3,\n",
    "                    n_steps=2048, batch_size=64, n_epochs=10,\n",
    "                    gamma=0.)\n",
    "Q, R = np.asarray([[1,0], [0,1]]), np.asarray([[0.00001]])\n",
    "angA, angB = np.pi/4, np.pi\n",
    "scalarA, scalarB = 0.8, 0.5\n",
    "xformA = lambda t: np.asarray([[np.cos(t*angA), -np.sin(t*angA)],\n",
    "                               [np.sin(t*angA), np.cos(t*angA)]]).T \\\n",
    "                    @ ((1-t)*np.eye(2) + t*scalarA*np.eye(2))\n",
    "xformB = lambda t: np.asarray([[np.cos(t*angB), -np.sin(t*angB)],\n",
    "                               [np.sin(t*angB), np.cos(t*angB)]]).T \\\n",
    "                    @ ((1-t)*np.eye(2) + t*scalarB*np.eye(2))\n",
    "x0 = np.asarray([-0.5, 0], np.float32)\n",
    "make_env = lambda: SpringMassEnv(**sys_kwargs, q=Q, r=R, seed=0)\n",
    "def make_xform_env(t):\n",
    "    env = make_env()\n",
    "    env.system.A = xformA(t) @ env.system.A\n",
    "    env.system.B = xformB(t) @ env.system.B\n",
    "    return env\n",
    "def make_xform_env(t):\n",
    "    env = make_env()\n",
    "    # env.system.A = NonLinearMatrix(env.system.A,\n",
    "    #                                make_xform(1-t, 1.))\n",
    "    env.system.A = xformA(t) @ env.system.A\n",
    "    env.system.B = xformB(t) @ env.system.B\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "sys = create_springmass(**sys_kwargs)\n",
    "interval = env.period * 25\n",
    "env_spr = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6560f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "make_xform_env(0.1).system.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72022f41",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_env_response(make_xform_env(0.9), np.asarray([0.4, 0]), agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc66450",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LanderEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c657fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = LanderEnv()\n",
    "    env.reset()\n",
    "    return env\n",
    "def make_xform_env(t):\n",
    "    min_power = 0.75\n",
    "    max_power = 1.\n",
    "    side = 1\n",
    "    relative_power = np.ones(2, np.float32)\n",
    "    relative_power[side] = min_power + (1-t) * (max_power - min_power)\n",
    "    env = LanderEnv(relative_power=relative_power)\n",
    "    env.reset()\n",
    "    return env\n",
    "learn_kwargs = dict(steps=500_000, seed=0, learning_rate=2e-3,\n",
    "                    n_steps=4096, batch_size=256, n_epochs=20,\n",
    "                    gamma=0.99)\n",
    "interval = 30\n",
    "x0 = None\n",
    "N = 6\n",
    "env_lander = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a15d2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = make_xform_env(0.)\n",
    "plot_env_response(env, None, agent,\n",
    "                 state_idx=(0,1,4), state_names='xya', max_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d557e98",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5057933",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from systems.lunarlander import LunarLanderEnv\n",
    "learn_kwargs = dict(steps=100_000, seed=0, learning_rate=5e-3,\n",
    "                    n_steps=1024, batch_size=256, n_epochs=5,\n",
    "                    gamma=0.99)\n",
    "def make_env():\n",
    "    env = LunarLanderEnv(seed=0)\n",
    "    env.reset()\n",
    "    return env\n",
    "def make_xform_env(t):\n",
    "    env = make_env()\n",
    "    min_power = 0.75\n",
    "    max_power = 1.\n",
    "    side = 1\n",
    "    relative_power = np.ones(2, np.float32)\n",
    "    env.relative_power[side] = min_power + (1-t) * (max_power - min_power)\n",
    "    env.reset()\n",
    "    return env\n",
    "x0 = None\n",
    "interval = 500\n",
    "env_lunar = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391cf1c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = make_xform_env(0.)\n",
    "plot_env_response(env, None, agent,\n",
    "                 state_idx=(0,1,4), state_names='xya')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59acb7",
   "metadata": {},
   "source": [
    "# Conditional Policy Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5f381",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b702d9f4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77c7a9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sort_by(arr, by):\n",
    "    res = [arr[i] for i in np.argsort(by)]\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        return np.asarray(res, dtype=arr.dtype)\n",
    "    return res\n",
    "\n",
    "def save_res(res, dest):\n",
    "    assert isinstance(res, SimpleNamespace), 'Must be SimpleNamespace'\n",
    "    try:\n",
    "        agents = res.agents\n",
    "        res.agents = []\n",
    "        with open(dest, 'wb') as f:\n",
    "            pickle.dump(res, f)\n",
    "    finally:\n",
    "        res.agents = agents\n",
    "def load_res(dest):\n",
    "    with open(dest, 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "    return res\n",
    "def save_agent(agent, dest):\n",
    "    agent.save(dest)\n",
    "def load_agent(dest):\n",
    "    agent = PPO.load(dest)\n",
    "    agent.policy.state_xform = agent.policy.state_xform.to(agent.policy.device)\n",
    "    agent.policy.action_xform = agent.policy.action_xform.to(agent.policy.device)\n",
    "    return agent\n",
    "def evaluate_trajectory(agent, env, x0=None, max_steps=np.inf):\n",
    "    if x0 is not None:\n",
    "        try:\n",
    "            env.reset(x0)\n",
    "        except:\n",
    "            print('Couldnt reset to x0')\n",
    "    else:\n",
    "        x0 = env.reset()\n",
    "    states, rewards, dones = [x0], [], [False]\n",
    "    step = 0\n",
    "    while not dones[-1] and step <= max_steps:\n",
    "        step += 1\n",
    "        action = agent.predict(states[-1])[0]\n",
    "        state, reward, done, *_ = env.step(action)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "    rewards.append(0.)\n",
    "    states = np.asarray(states, np.float32)\n",
    "    with torch.no_grad():\n",
    "        states_ = torch.from_numpy(states).to(agent.policy.device)\n",
    "        values = agent.policy.forward(states_)[1].detach().cpu().numpy().squeeze()\n",
    "    ret = np.asarray(returns(rewards, dones, learn_kwargs['gamma'], truncate=False))\n",
    "    return states.squeeze(), values, ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85ffce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_value(agent, *lims, policy=False):\n",
    "    # assert len(lims)==1\n",
    "    X = []\n",
    "    for lim in lims:\n",
    "        if not isinstance(lim, np.ndarray):\n",
    "            lim = np.linspace(*lim, num=25 if len(lim)<3 else lim[2])\n",
    "        X.append(lim.squeeze())\n",
    "    states = np.meshgrid(*X)\n",
    "    ogshape = states[0].shape\n",
    "    xins = [x.reshape(-1,1) for x in states]\n",
    "    xin = np.hstack(xins)\n",
    "    with torch.no_grad():\n",
    "        xin = torch.from_numpy(xin).float()\n",
    "        val = agent.policy.forward(xin)[1].detach().numpy()\n",
    "    V = val.reshape(*ogshape)\n",
    "    if len(lims)==1:\n",
    "        plt.plot(x, V)\n",
    "        plt.xlabel('State')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True, 'both', 'both')\n",
    "    elif len(lims)==2:\n",
    "        ax = plt.subplot(1,1,1, projection='3d')\n",
    "        ax.plot_surface(states[0], states[1], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf044d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reset_agent(agent, params):\n",
    "    agent.policy.load_state_dict(params)\n",
    "    agent.policy.state_xform *= 0\n",
    "    agent.policy.action_xform = torch.eye(len(agent.policy.action_xform))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc73e4b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0e97c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dpolicy_df(\n",
    "    A: np.ndarray, B: np.ndarray, F_A, F_B: np.ndarray,\n",
    "    x: np.ndarray, u\n",
    ") -> np.ndarray:\n",
    "    F_BB_ = torch.from_numpy(np.linalg.pinv(F_B @ B)).float()\n",
    "    A = torch.from_numpy(A).float()\n",
    "    B = torch.from_numpy(B).float()\n",
    "    F_A = torch.from_numpy(F_A).float()\n",
    "    F_B = torch.from_numpy(F_B).float()\n",
    "    x = torch.from_numpy(x).float()\n",
    "    u = torch.from_numpy(u).float()\n",
    "    I = torch.eye(len(A))\n",
    "    def pi(F_A):\n",
    "        return (F_BB_ @ ((I - F_A) @ A @ x.T + B @ u.T)).T\n",
    "    dpidfa = jacobian(pi, F_A)\n",
    "    # d2pidfa2 = hessian(pi, F_A)\n",
    "    # dpidfa = torch.einsum('bnij->bij', dpidfa)\n",
    "    def pi(F_B):\n",
    "        F_BB_ = torch.linalg.pinv(F_B @ B)\n",
    "        return (F_BB_ @ ((I - F_A) @ A @ x.T + B @ u.T)).T\n",
    "    dpidfb = jacobian(pi, F_B)\n",
    "    # d2pidfb2 = hessian(pi, F_B)\n",
    "    # dpidfb = torch.einsum('bnij->bij', dpidfb)\n",
    "    return dpidfa.numpy(), dpidfb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fea497",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sh(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(key, value.shape)\n",
    "def value_response(agent, A_s, B_s, F_A, F_B, x):\n",
    "    # dudfa, dudfb,*_ = dpolicy_df(A_s, B_s, F_A, F_B, x,\n",
    "    #                           agent.predict(x, deterministic=True)[0])\n",
    "    # sh(dudfa=dudfa, dudfb=dudfb)\n",
    "    dvdx, dudx = agent.policy.dvdpi_dobs(x, deterministic=True)\n",
    "    dvdx, dudx = dvdx.numpy(), dudx.numpy()\n",
    "    dxdu = np.stack([np.linalg.pinv(a) for a in dudx])\n",
    "    dvdu = np.matmul(dvdx, dxdu)\n",
    "    dvdut = np.matmul(dvdu,\n",
    "                      np.linalg.pinv(np.linalg.pinv(F_B@B_s)@B_s))\n",
    "    # dvdfa = (dvdu[:,None, :, :] * dudfa).squeeze(axis=1)\n",
    "    # dvdfb = dvdu[:,None, :, :] * dudfb).squeeze(axis=1)\n",
    "    dvdfa, dvdfb = None, None\n",
    "    obs = torch.from_numpy(np.atleast_2d(x)).float().to(agent.policy.device)\n",
    "    u_old = agent.policy.forward(obs, deterministic=True)[0].detach().cpu().numpy()\n",
    "    u_new = action_transform(x, u_old, A_s, B_s, F_A, F_B)\n",
    "    delta_u = u_new - u_old\n",
    "    delta_v = dvdu * delta_u[:, None, :] # batch, project axis, action\n",
    "    delta_vt = dvdut * delta_u[:, None, :] # batch, project axis, action\n",
    "    return (dvdfa, dvdfb,\n",
    "            dvdu, dvdut,\n",
    "            delta_v, delta_vt,\n",
    "            delta_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ebb786",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dist_v_g(val, ret):\n",
    "    val, ret = np.asarray(val), np.asarray(ret)\n",
    "    return np.linalg.norm(val-ret)\n",
    "\n",
    "def get_value_change(agent, continue_rl=False, N=6, interval=500):\n",
    "    res = SimpleNamespace()\n",
    "    res.dist, res.dist_xform, res.dist_new, res.dist_reused, res.agents = [], [], [], [], []\n",
    "    res.dfs_new, res.dfs_reused = [], []\n",
    "    res.infos = []\n",
    "    res.delta_v, res.delta_vt, res.dvdu, res.dvdut = [], [], [], []\n",
    "    res.delta_u = []\n",
    "    \n",
    "    for i in tqdm(np.arange(0, 1, 1/N), leave=False):\n",
    "        \n",
    "        _, val, ret = evaluate_trajectory(agent, make_xform_env(i), x0)\n",
    "        res.dist.append(np.linalg.norm(val-ret))\n",
    "\n",
    "        state_xform, action_xform, info = get_transforms(\n",
    "            agent, make_env(), make_xform_env(i),\n",
    "            n_episodes_or_steps='steps',\n",
    "            buffer_episodes=interval,\n",
    "            data_driven_source=True\n",
    "        )\n",
    "        agent_xform = transform_rl_policy(agent, state_xform, action_xform, copy=True)\n",
    "        res.infos.append(info)\n",
    "        \n",
    "        dvdfa, dvdfb, dvdu, dvdut, delta_v, delta_vt, delta_u = \\\n",
    "            value_response(\n",
    "                agent, info.A_s, info.B_s, info.F_A, info.F_B, info.x\n",
    "            )\n",
    "        res.delta_v.append(np.linalg.norm(delta_v, axis=2).squeeze())\n",
    "        res.delta_vt.append(np.linalg.norm(delta_vt, axis=2).squeeze())\n",
    "        res.dvdu.append(np.linalg.norm(dvdu, axis=2).squeeze())\n",
    "        res.dvdut.append(np.linalg.norm(dvdut, axis=2).squeeze())\n",
    "        \n",
    "        _, val, ret = evaluate_trajectory(agent_xform, make_xform_env(i), x0)\n",
    "        res.dist_xform.append(dist_v_g(val, ret))\n",
    "\n",
    "        res.agents.append([agent, agent_xform])\n",
    "        if continue_rl:\n",
    "            kwargs = learn_kwargs.copy()\n",
    "            kwargs['steps'] = continue_rl\n",
    "            dirname = 'temp/' + str(os.getpid()) + '/'\n",
    "            agent_new = learn_rl(make_xform_env(i),\n",
    "                                 reuse_parameters_of=agent_xform,\n",
    "                                 tensorboard_log=dirname+'new',\n",
    "                                 **kwargs)\n",
    "            res.dfs_new.append(get_tensorboard_scalar_frame('tensorboard/'+dirname+'new_1'))\n",
    "            _, val, ret = evaluate_trajectory(agent_xform, make_xform_env(i), x0)\n",
    "            res.dist_new.append(dist_v_g(val, ret))\n",
    "\n",
    "            agent_reused = learn_rl(make_xform_env(i),\n",
    "                                 reuse_parameters_of=agent,\n",
    "                                 tensorboard_log=dirname+'reused',\n",
    "                                 **kwargs)\n",
    "            res.dfs_reused.append(get_tensorboard_scalar_frame('tensorboard/'+dirname+'reused_1'))\n",
    "            _, val, ret = evaluate_trajectory(agent_reused, make_xform_env(i), x0)\n",
    "            res.dist_reused.append(dist_v_g(val, ret))\n",
    "            res.agents[-1].extend([agent_new, agent_reused])\n",
    "            shutil.rmtree('tensorboard/'+dirname, ignore_errors=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0654379",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load current_agent in case of notebook restart\n",
    "from stable_baselines3 import PPO\n",
    "agent = PPO.load('current_agent_'+make_env().name)\n",
    "agent.policy.state_xform = agent.policy.state_xform.to(agent.policy.device)\n",
    "agent.policy.action_xform = agent.policy.action_xform.to(agent.policy.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5221c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "agent = learn_rl(env, tensorboard_log=env.name+'/tuning', **learn_kwargs)\n",
    "agent.save('current_agent_'+make_env().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63603f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = get_value_change(agent, continue_rl=50_000, N=N, interval=500)\n",
    "save_res(res, 'res_'+make_env().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba73cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = load_res('res_'+make_env().name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c857c9a",
   "metadata": {},
   "source": [
    "### $V-G$ with env change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df = get_tensorboard_scalar_frame('tensorboard/'+make_env().name+'/tuning_18')\n",
    "prev = df['rollout', 'ep_rew_mean'].to_numpy()\n",
    "idx = df['rollout', 'ep_rew_mean'].index.to_numpy()\n",
    "\n",
    "new, reused = [], []\n",
    "for i in range(len(res.dfs_new)):\n",
    "    if res.dist[i] > 0:\n",
    "        new.append(res.dfs_new[i]['rollout', 'ep_rew_mean'].to_numpy())\n",
    "    else:\n",
    "        new.append(res.dfs_reused[i]['rollout', 'ep_rew_mean'].to_numpy())\n",
    "    reused.append(res.dfs_reused[i]['rollout', 'ep_rew_mean'].to_numpy())\n",
    "idx_new = res.dfs_new[0]['rollout', 'ep_rew_mean'].index.to_numpy()\n",
    "idx_new = idx_new + idx[-1]\n",
    "mn, sn = mean_std(new, axis=0)\n",
    "mr, sr = mean_std(reused, axis=0)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(idx, prev, label='Nominal RL')\n",
    "plt.axvline(x=idx[-1], c='r', ls='--', label='Fault')\n",
    "plt.plot(idx_new, mr, c='b', label='Tuned')\n",
    "plt.fill_between(idx_new, mr+sr, mr-sr, color='b', alpha=0.4)\n",
    "plt.plot(idx_new, mn, c='g', label='Transformed+Tuned')\n",
    "plt.fill_between(idx_new, mn+sn, mn-sn, color='g', alpha=0.4)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Training steps')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b412b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(6,3))\n",
    "improvement = [d-dn for d, dn in zip(res.dist, res.dist_new)]\n",
    "d_asc = sorted(res.dist)\n",
    "plt.plot(d_asc, sort_by(res.dist, res.dist), c='b', ls=':', label='$||V_{\\pi_s}-G_{\\pi_s}||$')\n",
    "plt.plot(d_asc, sort_by(res.dist_reused, res.dist), c='b', label='$||V_{\\pi_s}-G_{\\pi_s}||$ Tuned')\n",
    "plt.plot(d_asc, sort_by(res.dist_xform, res.dist), c='g', ls=':', lw='3',\n",
    "         label='$||V_{F_\\pi(\\pi_s)}-G_{F_\\pi(\\pi_s)}||$')\n",
    "plt.plot(d_asc, sort_by(res.dist_new, res.dist), c='g', lw='3',\n",
    "         label='$||V_{F_\\pi(\\pi_s)}-G_{F_\\pi(\\pi_s)}||$ Tuned')\n",
    "plt.ylabel('$||V - G||$ on $T_t$')\n",
    "plt.xlabel('$||V - G||$')\n",
    "# plt.xticks(np.arange(len(res.dist)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb5b60",
   "metadata": {},
   "source": [
    "### Quality of transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3459b86",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Sensitivity of value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf3c34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "mindv, maxdv = np.min(res.delta_v), np.max(res.delta_v)\n",
    "idx = np.argsort(res.dist)\n",
    "for i, dv in enumerate(res.delta_v):\n",
    "    plt.hist(res.delta_v[idx[i]], density=True, bins=20, range=(mindv, maxdv),\n",
    "             alpha=(1-i/N), label='$||V-G||=%.2f$' % res.dist[idx[i]])\n",
    "plt.legend()\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('$\\partial V /\\partial u \\cdot u$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3498a36",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot of gradient norms vs degrading system\n",
    "x = np.atleast_2d(x0)\n",
    "delta_vs, dvdus, dvduts = [], [], []\n",
    "xs = []\n",
    "for i in np.arange(0, 1, 1/N):\n",
    "    # reset_agent(agent, policy_params)\n",
    "    state_xform, action_xform, info = get_transforms(\n",
    "            agent=agent, env=make_env(), env_=make_xform_env(i),\n",
    "            buffer_episodes=500, n_episodes_or_steps='steps',\n",
    "            data_driven_source=False\n",
    "        )\n",
    "    dvdfa, dvdfb, dvdu, delta_v, delta_u, dvdut = value_response(\n",
    "        agent, info.A_s, info.B_s, info.F_A, info.F_B, info.x\n",
    "    )\n",
    "    dvdus.append(np.linalg.norm(dvdu, axis=2).squeeze())\n",
    "    dvduts.append(np.linalg.norm(dvdut, axis=2).squeeze())\n",
    "    delta_vs.append(delta_v.squeeze())\n",
    "    xs.append(info.x.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406511e",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "ax = plt.subplot(projection='3d' if len(res.infos[0].x[0])>=2 else None)\n",
    "xs = np.asarray(xs)\n",
    "idx = np.argsort(res.dist)\n",
    "for i in idx:\n",
    "    x, dv = res.infos[i].x, res.dvdut[i]\n",
    "    ax.scatter(x[::5][:,0], x[::5][:,1], dv[::5],\n",
    "               label='$||V-G||=%.2f$' % res.dist[i], alpha=0.6)\n",
    "plt.legend()\n",
    "ax.set_xlabel('$x_0$')\n",
    "if len(res.infos[0].x[0])==2:\n",
    "    ax.set_ylabel('$x_1$')\n",
    "    ax.set_zlabel('$||\\partial V / \\partial u$||')\n",
    "else:\n",
    "    ax.set_ylabel('$||\\partial V / \\partial u$||')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150bdb2",
   "metadata": {},
   "source": [
    "#### Correlation between improvement, $V-G$, $\\partial V / \\partial u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b8153",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "D, I, V, VT = [], [], [], []\n",
    "for t in trange(5, leave=False):\n",
    "    res_ = get_value_change(agent, continue_rl=False, N=10, interval=500)\n",
    "    D.append(sort_by(res_.dist, res_.dist))\n",
    "    I.append(sort_by([d-dn for d, dn in zip(res_.dist, res_.dist_xform)], res_.dist))\n",
    "    V.append([np.mean(a) for a in sort_by(res_.dvdu, res_.dist)])\n",
    "    VT.append([np.mean(a) for a in sort_by(res_.dvdut, res_.dist)])\n",
    "mD, sd = mean_std(D)\n",
    "mI, si = mean_std(I)\n",
    "mVT, svt = mean_std(VT)\n",
    "mV, sv = mean_std(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eacdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(mD, mI, c='c')\n",
    "plt.fill_between(mD, mI+si, mI-si, alpha=0.2)\n",
    "plt.ylabel('$\\Delta \\epsilon_G$', c='c')\n",
    "plt.xlabel('$||V_s-G_{\\pi_s,T_t}||$')\n",
    "plt.twinx()\n",
    "plt.plot(mD, mV, c='m', ls=':',\n",
    "        label='$\\partial V_s / \\partial \\pi_s$')\n",
    "plt.fill_between(mD, mV+sv, mV-sv, alpha=0.2, color='m')\n",
    "plt.plot(mD, mVT, c='m', ls='-',\n",
    "        label='$\\partial V_s / \\partial \\pi_t$')\n",
    "plt.fill_between(mD, mVT+svt, mVT-svt, alpha=0.2, color='m')\n",
    "plt.ylabel('$\\partial V_s / \\partial u$ on $T_t$', c='m')\n",
    "# plt.yscale('log')\n",
    "plt.legend(handles=plt.gca().lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f90a6",
   "metadata": {},
   "source": [
    "#### Correlation between $\\mathcal{D}$ and improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = np.arange(50, 2000, 200)\n",
    "\n",
    "D, I, V, VT = [], [], [], []\n",
    "for t in tqdm(intervals, leave=False):\n",
    "    res_ = get_value_change(agent, continue_rl=False, N=10, interval=t)\n",
    "    D.append(sort_by(res_.dist, res_.dist))\n",
    "    I.append(sort_by([d-dn for d, dn in zip(res_.dist, res_.dist_xform)], res_.dist))\n",
    "    V.append([np.mean(a) for a in sort_by(res_.dvdu, res_.dist)])\n",
    "    VT.append([np.mean(a) for a in sort_by(res_.dvdut, res_.dist)])\n",
    "\n",
    "I = np.asarray(I)\n",
    "mD, sd = mean_std(D, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae81e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.plot_surface(*np.meshgrid(intervals, mD, indexing='ij'),\n",
    "                np.clip(I, a_min=-100, a_max=None), cmap='coolwarm',\n",
    "               alpha=0.5)\n",
    "ax.set_xlabel('$\\mathcal{D}$')\n",
    "ax.set_ylabel('$V_s - G_{\\pi_s,T_t}$')\n",
    "ax.set_zlabel('$\\Delta \\epsilon_G$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b94afb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Approximation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39fca5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D, I, E_s, E_t = [], [], [], []\n",
    "intervals = np.arange(50, 1000, 200)\n",
    "for t in tqdm(intervals, leave=False):\n",
    "    res_ = get_value_change(agent, continue_rl=False, N=10, interval=t)\n",
    "    D.append(sort_by(res_.dist, res_.dist))\n",
    "    I.append(sort_by([d-dn for d, dn in zip(res_.dist, res_.dist_xform)], res_.dist))\n",
    "    E_s.append(sort_by([i.err_s for i in res_.infos], res_.dist))\n",
    "    E_t.append(sort_by([i.err_t for i in res_.infos], res_.dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb181f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mD, sd = mean_std(D)\n",
    "I = np.asarray(I)\n",
    "E = (np.asarray(E_s) + np.asarray(E_t)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cced9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "E_spr, E_lander, E_lunar = [], [], []\n",
    "intervals = [10, 20, 40, 80, 160, 320, 640, 1280]\n",
    "agent_spr = load_agent('current_agent_SpringMassEnv')\n",
    "agent_lander = load_agent('current_agent_LanderEnv')\n",
    "agent_lunar = load_agent('current_agent_LunarLanderEnv')\n",
    "for trial in trange(5, leave=False):\n",
    "    E_spr.append([])\n",
    "    E_lander.append([])\n",
    "    E_lunar.append([])\n",
    "    for i in intervals:\n",
    "        P, e, *_ = pseudo_matrix_from_data(env_spr, i, agent_spr, 'steps')\n",
    "        _, B, *_ = ab_xform_from_pseudo_matrix(P, None, 0.01)\n",
    "        E_spr[-1].append(e + err_inv(B))\n",
    "        P, e, *_ = pseudo_matrix_from_data(env_lander, i, agent_lander, 'steps')\n",
    "        _, B, *_ = ab_xform_from_pseudo_matrix(P, None, 0.1)\n",
    "        E_lander[-1].append(e + err_inv(B))\n",
    "        P, e, *_ = pseudo_matrix_from_data(env_lunar, i, agent_lunar, 'steps')\n",
    "        _, B, *_ = ab_xform_from_pseudo_matrix(P, None, 0.04)\n",
    "        E_lunar[-1].append(e + err_inv(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57365ce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mE_spr, sE_spr = mean_std(E_spr)\n",
    "mE_lander, sE_lander = mean_std(E_lander)\n",
    "mE_lunar, sE_lunar = mean_std(E_lunar)\n",
    "plt.figure(figsize=(6,3))\n",
    "l, = plt.plot(intervals, mE_spr, label='Spring-Mass system')\n",
    "plt.fill_between(intervals, mE_spr-sE_spr, mE_spr+sE_spr, color=l.get_color(), alpha=0.4)\n",
    "l, = plt.plot(intervals, mE_lander, label='Lander system')\n",
    "plt.fill_between(intervals, mE_lander-sE_lander, mE_lander+sE_lander, color=l.get_color(), alpha=0.4)\n",
    "l, = plt.plot(intervals, mE_lunar, label='Lunar lander system')\n",
    "plt.fill_between(intervals, mE_lunar-sE_lunar, mE_lunar+sE_lunar, color=l.get_color(), alpha=0.4)\n",
    "plt.ylim(bottom=-0.01, top=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('$\\epsilon_D + \\epsilon_i(B_t)$')\n",
    "plt.xlabel('$\\mathcal{D}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e0162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.plot_surface(*np.meshgrid(intervals, E, indexing='ij'),\n",
    "                np.clip(I, a_min=-100, a_max=None), cmap='coolwarm',\n",
    "               alpha=0.5)\n",
    "ax.set_xlabel('$\\mathcal{D}$')\n",
    "ax.set_ylabel('$\\epsilon_A$')\n",
    "ax.set_zlabel('$\\Delta \\epsilon_G$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44ab61",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a55a30",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# experiment config\n",
    "\n",
    "# Whether the knowledge of the source system is known,\n",
    "# or approximated from sampled experiences\n",
    "data_driven_source = True\n",
    "# Whether to assume that the system transformations are known\n",
    "# and not approximate\n",
    "accurate_xfer = not data_driven_source\n",
    "buffer_episodes = 5\n",
    "interval = 5 * env.period * buffer_episodes\n",
    "name = env.__class__.__name__\n",
    "if data_driven_source and not accurate_xfer:\n",
    "    name += 'StochasticAll'\n",
    "elif data_driven_source:\n",
    "    name += 'StochasticSource'\n",
    "elif not accurate_xfer:\n",
    "    name += 'StochasticXfer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309975d9",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train rl policy on original environment\n",
    "agent = learn_rl(make_env(), tensorboard_log=name+'/Source',\n",
    "                 **learn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0056902",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_env_response(make_xform_env(), x0, agent_xform_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a5dcb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "TODO change env_xforms so B!=0\n",
    "\n",
    "TODO diagnose steps > kwargs['steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3e9bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fine-tune source policy on target environment\n",
    "agent_new = learn_rl(make_xform_env(), tensorboard_log=name+'/Target',\n",
    "                 **learn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665ca1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fine-tine source policy on target environment\n",
    "agent_xform_tuned = concurrent_learn(agent, make_xform_env, interval,\n",
    "                             xform_policy=True,\n",
    "                             accurate_xfer=accurate_xfer,\n",
    "                             tensorboard_log=name+'/XformedTuned',\n",
    "                             **learn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e616e",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fine-tine the transformed policy, except xforms\n",
    "agent_xform_tuned = learn_rl(\n",
    "    make_xform_env(),\n",
    "    reuse_parameters_of=agent_xform,\n",
    "    learnable_transformation=False,\n",
    "    tensorboard_log=name+'/XformedTuned', **learn_kwargs\n",
    ")\n",
    "print('state_xform', agent_xform_tuned.policy.state_xform)\n",
    "print('action_xform', agent_xform_tuned.policy.action_xform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95eb04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # fine-tine the transformed policy, including xforms\n",
    "# agent_xform_tuned_all = learn_rl(\n",
    "#     make_xform_env(),\n",
    "#     reuse_parameters_of=agent_xform,\n",
    "#     learnable_transformation=True,\n",
    "#     tensorboard_log=name+'/XformedTunedAll', **learn_kwargs\n",
    "# )\n",
    "# print('state_xform', agent_xform_tuned_all.policy.state_xform.data)\n",
    "# print('action_xform', agent_xform_tuned_all.policy.action_xform.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cbb528",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Source policy on source task')\n",
    "print(evaluate_rl(agent, make_env(), n_eval_episodes=10))\n",
    "print('Reusing source policy')\n",
    "print(evaluate_rl(agent, make_xform_env(), n_eval_episodes=10))\n",
    "print('Tuning source policy')\n",
    "print(evaluate_rl(agent_new, make_xform_env(), n_eval_episodes=10))\n",
    "print('Transforming source policy')\n",
    "print(evaluate_rl(agent_xform, make_xform_env(), n_eval_episodes=10))\n",
    "print('Tuning transformed policy, except for transformations')\n",
    "print(evaluate_rl(agent_xform_tuned, make_xform_env(), n_eval_episodes=10))\n",
    "# print('Tuning transformed policy, including transformations')\n",
    "# print(evaluate_rl(agent_xform_tuned_all, make_xform_env(), n_eval_episodes=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e603d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816103ae",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multiple_response_plots([\n",
    "#     r'$\\pi_s$ on $P_s$ ',\n",
    "#     lambda: plot_env_response(make_env(), x0, agent),\n",
    "    r'$\\pi_s$ on $P_t$ ',\n",
    "    lambda: plot_env_response(make_xform_env(), x0, agent),\n",
    "    r'$\\pi_s^*$ on $P_t$ ',\n",
    "    lambda: plot_env_response(make_xform_env(), x0, agent_new, legend=False),\n",
    "    r'$\\pi_t$ on $P_t$ ',\n",
    "    lambda: plot_env_response(make_xform_env(), x0, agent_xform, legend=False),\n",
    "    r'$\\pi_t^-$ on $P_t$ ',\n",
    "    lambda: plot_env_response(make_xform_env(), x0, agent_xform_tuned, legend=False),\n",
    "    r'$\\pi_t^*$ on $P_t$ ',\n",
    "    lambda: plot_env_response(make_xform_env(), x0, agent_xform_tuned_all, legend=False)\n",
    "], figsize=(6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a075f29",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remember to specify up-to-date directory\n",
    "# name = env.__class__.__name__\n",
    "df = get_tensorboard_scalar_frame('tensorboard/%s/Source_1' % name)\n",
    "dft = get_tensorboard_scalar_frame('tensorboard/%s/Tuned_1' % name)\n",
    "dfxt = get_tensorboard_scalar_frame('tensorboard/%s/XformedTuned_1' % name)\n",
    "dfxta = get_tensorboard_scalar_frame('tensorboard/%s/XformedTunedAll_1' % name)\n",
    "\n",
    "%matplotlib inline\n",
    "last_tstep = df.index[-1]\n",
    "plt.figure(figsize=(6,2))\n",
    "for i, (frame, label) in enumerate([\n",
    "    (df, '$\\pi_s$ on $P_s$'),\n",
    "    (dft, '$\\pi_s^*$ on $P_t$'),\n",
    "    (dfxt, '$\\pi_t^-$ on $P_t$'),\n",
    "    (dfxta, '$\\pi_t^+$ on $P_t$')\n",
    "]):\n",
    "    if i > 0:\n",
    "        frame.index = frame.index + last_tstep\n",
    "    plt.plot(frame['rollout', 'ep_rew_mean'], label=label)\n",
    "if name.startswith('Simp'):\n",
    "    plt.legend()\n",
    "plt.ylabel('Mean episodic reward')\n",
    "plt.xlabel('Learning time steps')\n",
    "plt.setp(plt.xticks()[1], rotation=15)\n",
    "plt.grid(True, 'both')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d10df0e4cfad4a81e3051546436717ccc1eaea03864256b4a2f98229345d5e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
